{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b58558",
   "metadata": {},
   "source": [
    "Passo 1 - Extração dos arquivos do Twitter \n",
    "\n",
    "Notebook responsável por extrair os dados a partir de uma query configurável e salvar os dados em arquivos para processamento pelos demais passos do Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c11de",
   "metadata": {},
   "source": [
    "Setup de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5a17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import re\n",
    "import os\n",
    "import dataframe_image as dfi\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e38d57",
   "metadata": {},
   "source": [
    "Configuração de Token de Acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f80af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token='Insira o token de acesso', wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05e76b",
   "metadata": {},
   "source": [
    "Configuração de arquivos para salvamento dos dados e diretório padrão de armazenamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "rotulo = \"Nome do Arquivo\" # nome do arquivo que será usado como padrão para criação dos arquivos\n",
    "data_pesquisa = \"0101\" # data referencia usada para criação dos arquivos\n",
    "save_path = 'C:\\\\diretório\\\\' # diretório padrão para criação dos arquivos\n",
    "\n",
    "file_dado_raw_original = save_path + \"\\\\\" + rotulo + \"\\\\\" + rotulo + '_' + data_pesquisa + '_' + 'original.csv'\n",
    "file_dado_raw = save_path + \"\\\\\" + rotulo + \"\\\\\" + rotulo + '_CR_' + data_pesquisa + '_raw.csv'\n",
    "file_edges = save_path + \"\\\\\" + rotulo + \"\\\\\" + rotulo + '_CR_' + data_pesquisa + '_edges.csv'\n",
    "file_edges_full = save_path + \"\\\\\" + rotulo + \"\\\\\" + rotulo + '_CR_' + data_pesquisa + '_edges_full.csv'\n",
    "file_nodes = save_path + \"\\\\\" + rotulo + \"\\\\\" + rotulo + '_CR_' + data_pesquisa + '_nodes.csv'\n",
    "file_hist = save_path + \"\\\\\" + rotulo + \"\\\\\" + rotulo + '_CR_' + data_pesquisa + '_hist.png'\n",
    "\n",
    "print(\"Arquivo Original >\" + file_dado_raw_original)\n",
    "print(\"Arquivo Original Hidratado >\" + file_dado_raw)\n",
    "print(\"Arquivo Edges sem Tweets sem menção >\" + file_edges)\n",
    "print(\"Arquivo Edges com Tweets sem menção >\" + file_edges_full)\n",
    "print(\"Arquivo Nodes > \" + file_nodes)\n",
    "print(\"Arquivo Grafico Distr Tweets x tempo >\" + file_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34653914",
   "metadata": {},
   "source": [
    "Setup de query de extração dos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d48eac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Palavra chave para query lang:pt' # CR = com retweets SR = sem retweets\n",
    "Limite_ratio = 100000 # limite max tweets a ser recuperado \n",
    "inicio_time = '2023-01-01 T00:00:00Z' # a partir de que data e hora será baseada a pesquisa\n",
    "\n",
    "tweets = tweepy.Paginator(client.search_recent_tweets, start_time=inicio_time, query=query,tweet_fields=['id','created_at', 'author_id',  'conversation_id', 'entities', 'geo', 'in_reply_to_user_id', 'lang', 'public_metrics', 'referenced_tweets', 'reply_settings', 'context_annotations','withheld','source', 'text'], max_results=100).flatten(limit=Limite_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd671d0",
   "metadata": {},
   "source": [
    "Exemplos de tipos diferentes de queries - Usando data de Inicio e Fim para as pesquisas, buscando dados dos ultimos sete dias ou dados históricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc48dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inicio_time = '2023-07-24T00:00:00Z'\n",
    "#fim_time = '2023-07-24T120:00:00Z'\n",
    "\n",
    "#tweets = tweepy.Paginator(client.search_all_tweets, start_time=inicio_time, end_time=fim_time, query=query,tweet_fields=['id','created_at', 'author_id',  'conversation_id', 'entities', 'geo', 'in_reply_to_user_id', 'lang', 'public_metrics', 'referenced_tweets', 'reply_settings', 'context_annotations','withheld','source', 'text'], max_results=100).flatten(limit=Limite_ratio)\n",
    "#tweets = tweepy.Paginator(client.search_recent_tweets, start_time=inicio_time, end_time=fim_time, query=query,tweet_fields=['id','created_at', 'author_id',  'conversation_id', 'entities', 'geo', 'in_reply_to_user_id', 'lang', 'public_metrics', 'referenced_tweets', 'reply_settings', 'context_annotations','withheld','source', 'text'], max_results=100).flatten(limit=Limite_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc498af",
   "metadata": {},
   "source": [
    "Preparação dos arquivos a partir do Arquivo de Tweets original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cf9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [[tweet.created_at, tweet.author_id, tweet.text,tweet.conversation_id, tweet.entities, tweet.geo, tweet.in_reply_to_user_id, tweet.lang, tweet.public_metrics, tweet.referenced_tweets, tweet.reply_settings, tweet.context_annotations, tweet.withheld,tweet.source] for tweet in tweets]\n",
    "data3 = pd.DataFrame(data2, columns = [\"created_at\", \"author_id\", \"text\", \"conversation_id\", \"entities\", \"geo\", \"in_reply_to_user_id\", \"lang\", \"public_metrics\", \"referenced_tweets\", \"reply_settings\", \"context_annotations\", \"withheld\",\"source\"]) \n",
    "data3.to_csv(file_dado_raw_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94354901",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['timezone'] = 'America/Sao_Paulo'\n",
    "data3['created_at'] = pd.to_datetime(data3['created_at'])\n",
    "data3['Created_at_convert'] = data3['created_at'].dt.tz_convert(\"America/Sao_Paulo\")\n",
    "data3['temp_entities'] = data3['entities'].astype(str).str.extract(\"mentions': \\[(.*?)\\]\")\n",
    "data3 = data3.assign(var1=data3['temp_entities'].str.split(\"}, {\")).explode('var1')\n",
    "data3['mention_username'] = data3['var1'].astype(str).str.extract(\"'username': '(\\w+)\")\n",
    "data3['mention_userid'] = data3['var1'].astype(str).str.extract(\"'id': '(\\w+)\")\n",
    "data3['mention_username'] = data3['mention_username'].str.lower()\n",
    "data3 = data3.set_index(['created_at'])\n",
    "data3.to_csv(file_dado_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c4494",
   "metadata": {},
   "source": [
    "Geração de um gráfico de distribuição dos tweets ao longo da janela de tempo de extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb5c8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_hist = data3.hist(column='Created_at_convert',bins=8, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "\n",
    "plt.savefig(file_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1907c0",
   "metadata": {},
   "source": [
    "Geração do arquivo de Egdes para importação pela ferramenta Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13b35dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.DataFrame(data3, columns = [\"author_id\", \"mention_username\", 'Created_at_convert']) \n",
    "data4['Timeset'] = data4['Created_at_convert'].map(lambda x: x.isoformat())\n",
    "data4.rename(columns = {'author_id':'Source', 'mention_username':'Target'}, inplace = True)\n",
    "data4.Target = data4.Target.fillna('tweet_sem_menção')\n",
    "data4.to_csv(file_edges_full)\n",
    "data4 = data4[data4.Target.notnull()]\n",
    "data4.to_csv(file_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6ff9e",
   "metadata": {},
   "source": [
    "Geração do arquivo de Nodes para importação pela ferramenta Gephi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b4b72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes_data_source = pd.DataFrame(data3, columns = [\"author_id\", \"Label\", \"Tipo\", \"Created_at_convert\"]) \n",
    "Nodes_data_source['Timeset'] = Nodes_data_source['Created_at_convert'].map(lambda x: x.isoformat())\n",
    "Nodes_data_source = Nodes_data_source.drop_duplicates(subset=['author_id'])\n",
    "Nodes_data_Target = pd.DataFrame(data3, columns = [\"mention_username\", \"Label\", \"Tipo\", \"Created_at_convert\"]) \n",
    "Nodes_data_Target['Timeset'] = Nodes_data_Target['Created_at_convert'].map(lambda x: x.isoformat())\n",
    "Nodes_data_Target = Nodes_data_Target[Nodes_data_Target.mention_username.notnull()]\n",
    "Nodes_data_Target = Nodes_data_Target.drop_duplicates(subset=['mention_username'], keep = 'last')\n",
    "Nodes_data_source.rename(columns = {'author_id':'Id'}, inplace = True)\n",
    "Nodes_data_Target.rename(columns = {'mention_username':'Id'}, inplace = True)\n",
    "Nodes_data_Target['Id']=Nodes_data_Target['Id'].str.lower()\n",
    "frames = [Nodes_data_source, Nodes_data_Target]\n",
    "Nodes_frame = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18162310",
   "metadata": {},
   "source": [
    "Previa do Grafo que será gerado a partir dos dados extraídos, através da ferramenta Python. Serve para dar uma visão geral do que será melhor detalhado na ferramenta Gephi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf75fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grafo = nx.from_pandas_edgelist(\n",
    "    data4,\n",
    "    source=\"Source\",\n",
    "    target=\"Target\",\n",
    "    create_using=nx.DiGraph\n",
    ")\n",
    "\n",
    "print(nx.info(Grafo))\n",
    "\n",
    "grafo_nodes = Grafo.order()\n",
    "grafo_edges = Grafo.size()\n",
    "grafo_avg_deg = float(grafo_edges) / grafo_nodes\n",
    "grafo_componentes_forte_conectados = nx.number_strongly_connected_components(Grafo)\n",
    "grafo_componentes_fraco_conectados = nx.number_weakly_connected_components(Grafo)\n",
    "grafo_indegrees = Grafo.in_degree()\n",
    "\n",
    "print(\"Nodes:\",grafo_nodes)\n",
    "print(\"Edges:\",grafo_edges)\n",
    "print(\"Grau Médio:\", grafo_avg_deg)\n",
    "print(\"Qtde componentes com fraca conexao:\", grafo_componentes_fraco_conectados)\n",
    "print(\"Qtde componentes com forte conexao:\",grafo_componentes_forte_conectados)\n",
    "\n",
    "nnodes = Grafo.number_of_nodes()\n",
    "degrees_in = [d for n, d in Grafo.in_degree()]\n",
    "degrees_out = [d for n, d in Grafo.out_degree()]\n",
    "avrg_degree_in = sum(degrees_in) / float(nnodes)\n",
    "avrg_degree_out = sum(degrees_out) / float(nnodes)\n",
    "\n",
    "in_values = sorted(set(degrees_in))\n",
    "in_hist = [degrees_in.count(x) for x in in_values]\n",
    "out_values = sorted(set(degrees_out))\n",
    "out_hist = [degrees_out.count(x) for x in out_values]\n",
    "\n",
    "plt.figure()\n",
    "plt.loglog(in_values,in_hist)\n",
    "plt.loglog(out_values,out_hist)\n",
    "plt.legend(['In-degree','Out-degree'])\n",
    "plt.xlabel('Grau')\n",
    "plt.ylabel('Nro de Nodes')\n",
    "plt.title('Grafo')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c6209",
   "metadata": {},
   "source": [
    "Lista as contas que mais foram citadas nos tweets (inbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Liata_maiores_indegrees = sorted(Grafo.in_degree, key=lambda x: x[1], reverse=True)\n",
    "res_list_in = [x[0] for x in Liata_maiores_indegrees]\n",
    "res_list_in[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf42d6",
   "metadata": {},
   "source": [
    "Lista as contas que mais enviaram tweets (outbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Liata_maiores_outdegrees = sorted(Grafo.out_degree, key=lambda x: x[1], reverse=True)\n",
    "res_list_out = [x[0] for x in Liata_maiores_outdegrees]\n",
    "users_maiores_outdegrees = client.get_users(ids=res_list_out[:100], user_fields=['id', 'username'])\n",
    "users_maiores_outdegrees[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273ec16",
   "metadata": {},
   "source": [
    "Faz o merge com o arquivo de contas extraido a partir das listas de distribuição sobre as eleições, divulgadas pelo Twitter em 2022, onde foram extraídas os IDs, Nomes das contas, categoria de acordo com o nome da lista que a conta se encontrava (ex: Reporteres, Partidos Políticos, etc) e a declaração de voto de cada conta. A declaração se baseou na livre declaração de apoio aos candidatos Luis Inácio Lula da Silva e Jair Messias Bolsonaro. Caso essa declaração não tenha sido feita a conta era considerada como Neutra/Indefinida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data6 = pd.DataFrame(users_maiores_outdegrees.data, columns = [\"id\", \"name\", \"username\"])\n",
    "data6.rename(columns = {'id':'Id'}, inplace = True)\n",
    "Nodes_frame_final = pd.merge(Nodes_frame, data6, how = 'left', on = 'Id')\n",
    "Nodes_frame_final = Nodes_frame_final.reset_index(drop=True)\n",
    "arq_contas_brasil = 'twitter_contas_br_2022_v3.csv'\n",
    "df_contas_brasil = pd.read_csv(arq_contas_brasil, sep=';', low_memory = False, index_col=False)\n",
    "df_contas_brasil['Id']=df_contas_brasil['Id'].str.lower()\n",
    "Nodes_frame_final_categoria = pd.merge(Nodes_frame_final, df_contas_brasil, how='left', on=['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8632d",
   "metadata": {},
   "source": [
    "Geração do arquivo de Nodes para importação no Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60c5d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes_frame_final_categoria.to_csv(file_nodes, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de8fbf",
   "metadata": {},
   "source": [
    "Geração de uma da Nuvem de Palavras com os tweets que foram coletados pela query \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2cbbda8-78b8-4588-85e7-b1ebf105aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Lista_palavras = [\"vai\",\"agora\",\"HTTPS\",\"Se\", \"O\", \"ao\",\"de\",\"a\",\"o\",\"que\",\"e\",\"do\",\"da\",\"em\",\"um\",\"para\",\"é\",\"com\",\"não\",\"uma\",\"os\",\"no\",\"se\",\"na\",\"por\",\"mais\",\"as\",\"dos\",\"como\",\"mas\",\"foi\",\"ao\",\"ele\",\"das\",\"tem\",\"à\",\"seu\",\"sua\",\"ou\",\"ser\",\"quando\",\"muito\",\"há\",\"nos\",\"já\",\"está\",\"eu\",\"também\",\"só\",\"pelo\",\"pela\",\"até\",\"isso\",\"ela\",\"entre\",\"era\",\"depois\",\"sem\",\"mesmo\",\"aos\",\"ter\",\"seus\",\"quem\",\"nas\",\"me\",\"esse\",\"eles\",\"estão\",\"você\",\"tinha\",\"foram\",\"essa\",\"num\",\"nem\",\"suas\",\"meu\",\"às\",\"minha\",\"têm\",\"numa\",\"pelos\",\"elas\",\"havia\",\"seja\",\"qual\",\"será\",\"nós\",\"tenho\",\"lhe\",\"deles\",\"essas\",\"esses\",\"pelas\",\"este\",\"fosse\",\"dele\",\"tu\",\"te\",\"vocês\",\"vos\",\"lhes\",\"meus\",\"minhas\",\"teu\",\"tua\",\"teus\",\"tuas\",\"nosso\",\"nossa\",\"nossos\",\"nossas\",\"dela\",\"delas\",\"esta\",\"estes\",\"estas\",\"aquele\",\"aquela\",\"aqueles\",\"aquelas\",\"isto\",\"aquilo\",\"estou\",\"está\",\"estamos\",\"estão\",\"estive\",\"esteve\",\"estivemos\",\"estiveram\",\"estava\",\"estávamos\",\"estavam\",\"estivera\",\"estivéramos\",\"esteja\",\"estejamos\",\"estejam\",\"estivesse\",\"estivéssemos\",\"estivessem\",\"estiver\",\"estivermos\",\"estiverem\",\"hei\",\"há\",\"havemos\",\"hão\",\"houve\",\"houvemos\",\"houveram\",\"houvera\",\"houvéramos\",\"haja\",\"hajamos\",\"hajam\",\"houvesse\",\"houvéssemos\",\"houvessem\",\"houver\",\"houvermos\",\"houverem\",\"houverei\",\"houverá\",\"houveremos\",\"houverão\",\"houveria\",\"houveríamos\",\"houveriam\",\"sou\",\"somos\",\"são\",\"era\",\"éramos\",\"eram\",\"fui\",\"foi\",\"fomos\",\"foram\",\"fora\",\"fôramos\",\"seja\",\"sejamos\",\"sejam\",\"fosse\",\"fôssemos\",\"fossem\",\"for\",\"formos\",\"forem\",\"serei\",\"será\",\"seremos\",\"serão\",\"seria\",\"seríamos\",\"seriam\",\"tenho\",\"tem\",\"temos\",\"tém\",\"tinha\",\"tínhamos\",\"tinham\",\"tive\",\"teve\",\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\"tenha\",\"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivesse\",\"tiver\",\"tivermos\",\"tiverem\",\"terei\",\"terá\",\"teremos\",\"terão\",\"teria\",\"teríamos\",\"teriam\",\"Se\",\"RT\",\"se\",\"ao\"]\n",
    "\n",
    "text = \" \".join(i for i in data3.text)\n",
    "\n",
    "stop_words = Lista_palavras + list(STOPWORDS)\n",
    "\n",
    "regex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "\n",
    "text_limpo_a = re.sub(regex_pattern,'',text) #replaces pattern with ''\n",
    "\n",
    "re_list = ['@[A-Za-z0–9_]+', '#']\n",
    "combined_re = re.compile( '|'.join( re_list) )\n",
    "text_limpo_b = re.sub(combined_re,'',text_limpo_a)\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(max_words=50, collocation_threshold = 10, width = 1500, height = 1000, random_state = 42, collocations = False, colormap = 'tab10', stopwords = stop_words, background_color=\"white\").generate(text_limpo_b)\n",
    "\n",
    "plt.figure( figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
